{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "liwcPath = 'liwc/liwc-english-mod.dic'\n",
    "\n",
    "TRANSLATE_TABLE = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "\n",
    "class LIWC():\n",
    "    \"\"\"Top-level class\"\"\"\n",
    "\n",
    "    def __init__(self, dict_path):\n",
    "        self.lexicon, self.category_names = self._read_dic(dict_path)\n",
    "        self.trie = self._build_trie(self.lexicon)\n",
    "\n",
    "    def process_text(self, text):\n",
    "        \"\"\"Run LIWC on string\"\"\"\n",
    "\n",
    "        tokenized = word_tokenize(text.lower().translate(TRANSLATE_TABLE))\n",
    "        counts = defaultdict(int)\n",
    "        dict_count = len(tokenized)\n",
    "\n",
    "        for token in tokenized:\n",
    "            classifications = list(self._parse_token(token))\n",
    "\n",
    "            if not classifications:\n",
    "                dict_count -= 1\n",
    "            else:\n",
    "                for category in classifications:\n",
    "                    counts[category] += 1\n",
    "\n",
    "        category_scores = {category: (\n",
    "            counts[category] / len(tokenized)) * 100 for category in counts.keys()}\n",
    "\n",
    "        return category_scores\n",
    "\n",
    "    def process_df_mp(self, df, col):\n",
    "        \"\"\"Multi-process version of process_df\"\"\"\n",
    "        cpu_count = mp.cpu_count()\n",
    "        p = mp.Pool(cpu_count)\n",
    "\n",
    "        batches = np.array_split(df, cpu_count)\n",
    "\n",
    "        pool_results = p.starmap(self.process_df,[(batch, col) for batch in batches if len(batch) > 0])\n",
    "        p.close()\n",
    "        \n",
    "        return pd.concat(pool_results, axis=0)\n",
    "\n",
    "    def process_df(self, df, col):\n",
    "        \"\"\"Run LIWC on a dataframe column\"\"\"\n",
    "        df[col] = df[col].astype(str)\n",
    "\n",
    "        def apply_df(row, col):\n",
    "            score = self.process_text(row[col])\n",
    "            scores = {}\n",
    "            \n",
    "            for category in score:\n",
    "                scores[category] = score[category]\n",
    "\n",
    "            return pd.Series(scores)\n",
    "\n",
    "\n",
    "        res = df.apply(apply_df, args=(col,), axis=1)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def _read_dic(self, filepath):\n",
    "        category_mapping = {}\n",
    "        category_names = []\n",
    "        lexicon = {}\n",
    "        mode = 0    # the mode is incremented by each '%' line in the file\n",
    "        with open(filepath) as dict_file:\n",
    "            for line in dict_file:\n",
    "                tsv = line.strip()\n",
    "                if tsv:\n",
    "                    parts = tsv.split('\\t')\n",
    "                    if parts[0] == '%':\n",
    "                        mode += 1\n",
    "                    elif mode == 1:\n",
    "                        # definining categories\n",
    "                        category_names.append(parts[1])\n",
    "                        category_mapping[parts[0]] = parts[1]\n",
    "                    elif mode == 2:\n",
    "                        lexicon[parts[0]] = [category_mapping[category_id]\n",
    "                                             for category_id in parts[1:]]\n",
    "        return lexicon, category_names\n",
    "\n",
    "    def _build_trie(self, lexicon):\n",
    "        '''\n",
    "        Build a character-trie from the plain pattern_string -> categories_list\n",
    "        mapping provided by `lexicon`.\n",
    "\n",
    "        Some LIWC patterns end with a `*` to indicate a wildcard match.\n",
    "        '''\n",
    "        trie = {}\n",
    "        for pattern, category_names in lexicon.items():\n",
    "            cursor = trie\n",
    "            for char in pattern:\n",
    "                if char == '*':\n",
    "                    cursor['*'] = category_names\n",
    "                    break\n",
    "                if char not in cursor:\n",
    "                    cursor[char] = {}\n",
    "                cursor = cursor[char]\n",
    "            cursor['$'] = category_names\n",
    "        return trie\n",
    "\n",
    "    def _search_trie(self, trie, token, token_i=0):\n",
    "        '''\n",
    "        Search the given character-trie for paths that match the `token` string.\n",
    "        '''\n",
    "        if '*' in trie:\n",
    "            return trie['*']\n",
    "        elif '$' in trie and token_i == len(token):\n",
    "            return trie['$']\n",
    "        elif token_i < len(token):\n",
    "            char = token[token_i]\n",
    "            if char in trie:\n",
    "                return self._search_trie(trie[char], token, token_i + 1)\n",
    "        return []\n",
    "\n",
    "    def _parse_token(self, token):\n",
    "        for category_name in self._search_trie(self.trie, token):\n",
    "            yield category_name\n",
    "            \n",
    "            \n",
    "''' \n",
    "Make LIWC feature extractor into class\n",
    "'''\n",
    "\n",
    "\n",
    "def makeLIWCDictionary(liwcPath, picklePath):\n",
    "    '''\n",
    "        Make lookup data structure from LIWC dictionary file\n",
    "    '''\n",
    "    LIWC_file = open(liwcPath, 'rb') # LIWC dictionary\n",
    "    catNames = {}\n",
    "    LIWC_file.readline() #skips first '%' line\n",
    "    line = LIWC_file.readline()\n",
    "    lookup = []\n",
    "    while '%' not in line:\n",
    "        keyval = line.split('\\t')\n",
    "        key = keyval[0]\n",
    "        value = keyval[1].strip()\n",
    "        catNames[key] = {'name' : value,\n",
    "                         'words' : []}\n",
    "        line = LIWC_file.readline()\n",
    "    mapCategoriesToNumbers = catNames.keys()\n",
    "    line = LIWC_file.readline() # skips second '%' line\n",
    "\n",
    "    #return mapCategoriesToNumbers\n",
    "    while line: #iterate through categories\n",
    "        data = line.strip().split('\\t')\n",
    "        reString = '^'+data[0].replace('*', '.*') + '$'\n",
    "        indeces = [mapCategoriesToNumbers.index(d) for d in data[1:]]\n",
    "        lookupCell = (re.compile(reString), indeces)\n",
    "        lookup.append(lookupCell)\n",
    "        for cat in data[1:]:\n",
    "            catNames[cat]['words'] += (data[0], reString)\n",
    "        cats = data[1:]\n",
    "        line = LIWC_file.readline()\n",
    "    toPickle = {'categories' : catNames, 'lookup' : lookup, 'cat_to_num' : mapCategoriesToNumbers}\n",
    "    pickle.dump(toPickle, open(picklePath, 'w'))\n",
    "    return toPickle\n",
    "\n",
    "class liwcExtractor():\n",
    "    def __init__(self,\n",
    "                tokenizer=None,\n",
    "                ignore=None,\n",
    "                dictionary=None,\n",
    "                newCategories=None,\n",
    "                keepNonDict=True,\n",
    "                liwcPath=None):\n",
    "        self.liwcPath = liwcPath\n",
    "        self.dictionary = dictionary\n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = self.nltk_tokenize\n",
    "        if liwcPath is not None:\n",
    "            self.dictionary = makeLIWCDictionary(liwcPath, './liwcDictionary.pickle')\n",
    "            self.lookup = self.dictionary['lookup']\n",
    "            self.categories = self.dictionary['categories']\n",
    "            self.mapCategoriesToNumbers = self.dictionary['cat_to_num']\n",
    "        elif self.dictionary==None:\n",
    "            self.dictionary = makeLIWCDictionary(liwcPath, './liwcDictionary.pickle')\n",
    "            self.lookup = self.dictionary['lookup']\n",
    "            self.categories = self.dictionary['categories']\n",
    "            self.mapCategoriesToNumbers = self.dictionary['cat_to_num']\n",
    "        self.ignore = ignore\n",
    "        self.newCategories = newCategories\n",
    "        self.nonDictTokens = []\n",
    "        self.keepNonDict = keepNonDict\n",
    "\n",
    "    def getCategoryIndeces(self):\n",
    "        indeces = [x['name'] for x in self.categories.values()]\n",
    "        indeces += ['wc', 'sixltr','dic','punc','emoticon'] # These last two are not built yet.\n",
    "        return indeces\n",
    "\n",
    "    def extract(self, corpus):\n",
    "        corpusFeatures = []\n",
    "        for doc in corpus:\n",
    "            features = self.extractFromDoc(doc)\n",
    "            corpusFeatures.append(features)\n",
    "        return corpusFeatures\n",
    "\n",
    "    def extractFromDoc(self, document):\n",
    "        tokens = self.tokenizer(document)\n",
    "        #print tokens\n",
    "        features = [0] * 70 # 66 = wc, total word count\n",
    "                            # 67 = sixltr, six letter words\n",
    "                            # 68 = dic, words found in LIWC dictionary\n",
    "                            # 70 = punc, punctuation\n",
    "                            # 71 = emoticon\n",
    "        features[66] = len(tokens)\n",
    "\n",
    "        for t in tokens: #iterating through tokens of a message\n",
    "            #print \"Token : \" + t\n",
    "            if len(t) > 6: # check if more than six letters\n",
    "                features[67] += 1\n",
    "            inDict = False\n",
    "            for pattern, categories in self.lookup:\n",
    "                if len(pattern.findall(t)) > 0:\n",
    "                    inDict = True\n",
    "                    for c in categories:\n",
    "                        features[int(c)] += 1\n",
    "            if inDict:\n",
    "                features[68] += 1\n",
    "            else:\n",
    "                self.nonDictTokens.append(t)\n",
    "        return features\n",
    "\n",
    "    def patternsMatchedFromDoc(self, document):\n",
    "        tokens = self.tokenizer(document)\n",
    "        patterns = [l[0] for l in self.lookup]\n",
    "        features = [0] * len(patterns)\n",
    "        for t in tokens:\n",
    "            for i, pattern in enumerate(patterns):\n",
    "                if len(pattern.findall(t)) > 0:\n",
    "                    features[i] += 1\n",
    "        return features\n",
    "\n",
    "    def nltk_tokenize(self, message):\n",
    "        '''\n",
    "            takes in a text string and returns a list of tokenized words using nltk methods\n",
    "        '''\n",
    "        # sentence tokenize\n",
    "        stList = nltk.sent_tokenize(message)\n",
    "        # word tokenize\n",
    "        tokens = []\n",
    "        for sent in stList:\n",
    "            tokens += nltk.word_tokenize(sent)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "path = r'C://Users/shiva/Desktop/RaviKrishna/txt/' # use your path\n",
    "all_files = glob.glob(path + \"*.txt\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    disorder_type = str(filename[40:-4])\n",
    "    df = pd.read_csv(filename, sep=\"\\n\", header=None,error_bad_lines=False,encoding='utf8', quoting=csv.QUOTE_NONE)\n",
    "    df['Disorder Type'] = disorder_type \n",
    "    li.append(df)\n",
    "tweetsDF = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame (1916, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Disorder Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have an assessment for therapy later. I went...</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can GENERALIZED ANXIETY DISORDER lead to suici...</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en dit both ways. mensen met mental illness ze...</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@BluemoonShell Yup. In the United States, we d...</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@iamWalkerR Heard thats a symtom from TAD: Tru...</td>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets Disorder Type\n",
       "0  I have an assessment for therapy later. I went...            AD\n",
       "1  Can GENERALIZED ANXIETY DISORDER lead to suici...            AD\n",
       "2  en dit both ways. mensen met mental illness ze...            AD\n",
       "3  @BluemoonShell Yup. In the United States, we d...            AD\n",
       "4  @iamWalkerR Heard thats a symtom from TAD: Tru...            AD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.columns = ['Tweets', 'Disorder Type']\n",
    "print(\"Shape of the DataFrame\", tweetsDF.shape)\n",
    "tweetsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "\n",
    "def analize_sentiment(tweet):\n",
    "    # Simple implementation of the sgn(x) function to make the analysis more comprenesive. \n",
    "    \n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive tweets: 29.07098121085595%\n",
      "Percentage of neutral tweets: 49.26931106471816%\n",
      "Percentage of negative tweets: 21.659707724425886%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Disorder Type</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have an assessment for therapy later. I went...</td>\n",
       "      <td>AD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can GENERALIZED ANXIETY DISORDER lead to suici...</td>\n",
       "      <td>AD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en dit both ways. mensen met mental illness ze...</td>\n",
       "      <td>AD</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@BluemoonShell Yup. In the United States, we d...</td>\n",
       "      <td>AD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@iamWalkerR Heard thats a symtom from TAD: Tru...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@bongwatersoda social anxiety, AvPD, BPD, bipo...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Reinecke: interested in understanding which su...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>More than 10,500 police officers across the UK...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mental Health Monday - Anxiety Disorder sympto...</td>\n",
       "      <td>AD</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Maternal mental health problems have been asso...</td>\n",
       "      <td>AD</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I can't quite believe it but I will graduating...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>find it helpful talking openly to some friends...</td>\n",
       "      <td>AD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Im pretty sure my mom has some form of anxiety...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I can't exercise because of my asthma and join...</td>\n",
       "      <td>AD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@DoYouEvenLIf You even used your 99 year old g...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Good Reasons for Bad Feelings\" book review a ...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Research has shown that magnesium deficiency i...</td>\n",
       "      <td>AD</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Persistent Depressive Disorder &amp;amp; Anxiety D...</td>\n",
       "      <td>AD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>For the minority of people with seasonal affec...</td>\n",
       "      <td>AD</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@zelinxia Gamer with an anxiety disorder re-en...</td>\n",
       "      <td>AD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweets Disorder Type  Sentiment\n",
       "0   I have an assessment for therapy later. I went...            AD          0\n",
       "1   Can GENERALIZED ANXIETY DISORDER lead to suici...            AD          0\n",
       "2   en dit both ways. mensen met mental illness ze...            AD         -1\n",
       "3   @BluemoonShell Yup. In the United States, we d...            AD          0\n",
       "4   @iamWalkerR Heard thats a symtom from TAD: Tru...            AD          1\n",
       "5   @bongwatersoda social anxiety, AvPD, BPD, bipo...            AD          1\n",
       "6   Reinecke: interested in understanding which su...            AD          1\n",
       "7   More than 10,500 police officers across the UK...            AD          1\n",
       "8   Mental Health Monday - Anxiety Disorder sympto...            AD         -1\n",
       "9   Maternal mental health problems have been asso...            AD         -1\n",
       "10  I can't quite believe it but I will graduating...            AD          1\n",
       "11  find it helpful talking openly to some friends...            AD          0\n",
       "12  Im pretty sure my mom has some form of anxiety...            AD          1\n",
       "13  I can't exercise because of my asthma and join...            AD          0\n",
       "14  @DoYouEvenLIf You even used your 99 year old g...            AD          1\n",
       "15  \"Good Reasons for Bad Feelings\" book review a ...            AD          1\n",
       "16  Research has shown that magnesium deficiency i...            AD         -1\n",
       "17  Persistent Depressive Disorder &amp; Anxiety D...            AD          0\n",
       "18  For the minority of people with seasonal affec...            AD         -1\n",
       "19  @zelinxia Gamer with an anxiety disorder re-en...            AD          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "tweetsDF['Sentiment'] = np.array([ analize_sentiment(tweet) for tweet in tweetsDF['Tweets'] ])\n",
    "\n",
    "pos_tweets = [ tweet for index, tweet in enumerate(tweetsDF['Tweets']) if tweetsDF['Sentiment'][index] > 0]\n",
    "neu_tweets = [ tweet for index, tweet in enumerate(tweetsDF['Tweets']) if tweetsDF['Sentiment'][index] == 0]\n",
    "neg_tweets = [ tweet for index, tweet in enumerate(tweetsDF['Tweets']) if tweetsDF['Sentiment'][index] < 0]\n",
    "\n",
    "print(\"Percentage of positive tweets: {}%\".format(len(pos_tweets)*100/len(tweetsDF['Tweets'])))\n",
    "print(\"Percentage of neutral tweets: {}%\".format(len(neu_tweets)*100/len(tweetsDF['Tweets'])))\n",
    "print(\"Percentage of negative tweets: {}%\".format(len(neg_tweets)*100/len(tweetsDF['Tweets'])))\n",
    "\n",
    "display(tweetsDF.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define more helper functions\n",
    "def preprocess_tweet(tweet):\n",
    "    #Preprocess the text in a single tweet\n",
    "    #arguments: tweet = a single tweet in form of string \n",
    "    #convert the tweet to lower case\n",
    "    tweet.lower()\n",
    "    #convert all urls to sting \"URL\"\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #convert all @username to \"\"\n",
    "    tweet = re.sub('@[^\\s]+','', tweet)\n",
    "    #correct all multiple white spaces to a single white space\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #convert \"#topic\" to just \"topic\"\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "# Clean a tweet\n",
    "def clean_tweet(text):\n",
    "    # Removal of URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Removal of mentions\n",
    "    text = re.sub(\"@[^\\s]*\", \"\", text)\n",
    "    # Removal of hashtags\n",
    "    text = re.sub(\"#[^\\s]*\", \"\", text)\n",
    "    # Removal of numbers\n",
    "    text = re.sub('[0-9]*[+-:]*[0-9]+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Apostrophe lookup\n",
    "    text = re.sub(\"'ll\", \" will\", text)\n",
    "    text = re.sub(\"'ve\", \" have\", text)\n",
    "    text = re.sub(\"n't\", \" not\", text)\n",
    "    text = re.sub(\"'d\", \" would\", text)\n",
    "    text = re.sub(\"'re\", \" are\", text)\n",
    "    text = re.sub(\"i'm\", \"i am\", text)\n",
    "    text = re.sub(\"it's\", \"it is\", text)\n",
    "    text = re.sub(\"she's\", \"she is\", text)\n",
    "    text = re.sub(\"he's\", \"he is\", text)\n",
    "    text = re.sub(\"here's\", \"here is\", text)\n",
    "    text = re.sub(\"that's\", \"that is\", text)\n",
    "    text = re.sub(\"there's\", \"there is\", text)\n",
    "    text = re.sub(\"what's\", \"what is\", text)\n",
    "    text = re.sub(\"who's\", \"who is\", text)\n",
    "    text = re.sub(\"'s\", \"\", text)\n",
    "    # Handling slang words\n",
    "    text = re.sub(r\"\\btmrw\\b\", \"tomorrow\", text)\n",
    "    text = re.sub(r\"\\bur\\b\", \"your\", text)\n",
    "    text = re.sub(r\"\\burs\\b\", \"yours\", text)\n",
    "    text = re.sub(r\"\\bppl\\b\", \"people\", text)\n",
    "    text = re.sub(r\"\\byrs\\b\", \"years\", text)\n",
    "    # Handling acronyms\n",
    "    text = re.sub(r\"\\b(rt)\\b\", \"retweet\", text)\n",
    "    text = re.sub(r\"\\b(btw)\\b\", \"by the way\", text)\n",
    "    text = re.sub(r\"\\b(asap)\\b\", \"as soon as possible\", text)\n",
    "    text = re.sub(r\"\\b(fyi)\\b\", \"for your information\", text)\n",
    "    text = re.sub(r\"\\b(tbt)\\b\", \"throwback thursday\", text)\n",
    "    text = re.sub(r\"\\b(tba)\\b\", \"to be announced\", text)\n",
    "    text = re.sub(r\"\\b(tbh)\\b\", \"to be honest\", text)\n",
    "    text = re.sub(r\"\\b(faq)\\b\", \"frequently asked questions\", text)\n",
    "    text = re.sub(r\"\\b(icymi)\\b\", \"in case you missed it\", text)\n",
    "    text = re.sub(r\"\\b(aka)\\b\", \"also known as\", text)\n",
    "    text = re.sub(r\"\\b(ama)\\b\", \"ask me anything\", text)\n",
    "    # Word lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "#df['SentimentText'] = df['SentimentText'].apply(lambda text: clean_tweet(text))\n",
    "\n",
    "def feature_extraction(data, method = \"tfidf\"):\n",
    "    #arguments: data = all the tweets in the form of array, method = type of feature extracter\n",
    "    #methods of feature extractions: \"tfidf\" and \"doc2vec\"\n",
    "    if method == \"tfidf\":\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        tfv=TfidfVectorizer(sublinear_tf=True, stop_words = \"english\") # we need to give proper stopwords list for better performance\n",
    "        features=tfv.fit_transform(data)\n",
    "    elif method == \"doc2vec\":\n",
    "        None\n",
    "    else:\n",
    "        return \"Incorrect inputs\"\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from html.parser import HTMLParser\n",
    "from nltk import WordNetLemmatizer\n",
    "tweetsDF['Cleaned Tweets'] = tweetsDF['Tweets'].apply(preprocess_tweet)\n",
    "tweetsDF['Cleaned Tweets'] = tweetsDF['Tweets'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Disorder Type</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Cleaned Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have an assessment for therapy later. I went...</td>\n",
       "      <td>AD</td>\n",
       "      <td>0</td>\n",
       "      <td>i have an assessment for therapy later. i went...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can GENERALIZED ANXIETY DISORDER lead to suici...</td>\n",
       "      <td>AD</td>\n",
       "      <td>0</td>\n",
       "      <td>can generalized anxiety disorder lead to suicide?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en dit both ways. mensen met mental illness ze...</td>\n",
       "      <td>AD</td>\n",
       "      <td>-1</td>\n",
       "      <td>en dit both ways. mensen met mental illness ze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@BluemoonShell Yup. In the United States, we d...</td>\n",
       "      <td>AD</td>\n",
       "      <td>0</td>\n",
       "      <td>yup. in the united states, we don’t have pda, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@iamWalkerR Heard thats a symtom from TAD: Tru...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "      <td>heard thats a symtom from tad: trump anxiety d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@bongwatersoda social anxiety, AvPD, BPD, bipo...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "      <td>social anxiety, avpd, bpd, bipolar disorder, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Reinecke: interested in understanding which su...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "      <td>reinecke: interested in understanding which su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>More than 10,500 police officers across the UK...</td>\n",
       "      <td>AD</td>\n",
       "      <td>1</td>\n",
       "      <td>more than police officer across the uk took ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mental Health Monday - Anxiety Disorder sympto...</td>\n",
       "      <td>AD</td>\n",
       "      <td>-1</td>\n",
       "      <td>mental health monday - anxiety disorder sympto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Maternal mental health problems have been asso...</td>\n",
       "      <td>AD</td>\n",
       "      <td>-1</td>\n",
       "      <td>maternal mental health problem have been assoc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets Disorder Type  Sentiment  \\\n",
       "0  I have an assessment for therapy later. I went...            AD          0   \n",
       "1  Can GENERALIZED ANXIETY DISORDER lead to suici...            AD          0   \n",
       "2  en dit both ways. mensen met mental illness ze...            AD         -1   \n",
       "3  @BluemoonShell Yup. In the United States, we d...            AD          0   \n",
       "4  @iamWalkerR Heard thats a symtom from TAD: Tru...            AD          1   \n",
       "5  @bongwatersoda social anxiety, AvPD, BPD, bipo...            AD          1   \n",
       "6  Reinecke: interested in understanding which su...            AD          1   \n",
       "7  More than 10,500 police officers across the UK...            AD          1   \n",
       "8  Mental Health Monday - Anxiety Disorder sympto...            AD         -1   \n",
       "9  Maternal mental health problems have been asso...            AD         -1   \n",
       "\n",
       "                                      Cleaned Tweets  \n",
       "0  i have an assessment for therapy later. i went...  \n",
       "1  can generalized anxiety disorder lead to suicide?  \n",
       "2  en dit both ways. mensen met mental illness ze...  \n",
       "3  yup. in the united states, we don’t have pda, ...  \n",
       "4  heard thats a symtom from tad: trump anxiety d...  \n",
       "5  social anxiety, avpd, bpd, bipolar disorder, a...  \n",
       "6  reinecke: interested in understanding which su...  \n",
       "7  more than police officer across the uk took ti...  \n",
       "8  mental health monday - anxiety disorder sympto...  \n",
       "9  maternal mental health problem have been assoc...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc = LIWC(\"LIWC2015_English_Flat.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_df = liwc.process_df(tweetsDF, col='Cleaned Tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "liwc_df.to_csv('LIWC_features.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Extra Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def adding_extra_feature(df, tweet_column):\n",
    "    \n",
    "    # Print Number of Exclamation\n",
    "    #length_of_excl = (len(re.findall(r'!', string)))\n",
    "    df['number_of_exclamation'] = tweet_column.apply(lambda x: (len(re.findall(r'!', x))))\n",
    "    \n",
    "    # Number of ?\n",
    "    #length_of_questionmark = (len(re.findall(r'?', string)))\n",
    "    df['number_of_questionmark'] = tweet_column.apply(lambda x: (len(re.findall(r'[?]', x))))\n",
    "    \n",
    "    # Number of #\n",
    "    df['number_of_hashtag'] = tweet_column.apply(lambda x: (len(re.findall(r'#', x))))\n",
    "    \n",
    "    # Number of @\n",
    "    df['number_of_mention'] = tweet_column.apply(lambda x: (len(re.findall(r'@', x))))\n",
    "    \n",
    "    # Number of Quotes\n",
    "    df['number_of_quotes'] = tweet_column.apply(lambda x: (len(re.findall(r\"'\", x))))\n",
    "\n",
    "    # Number if underscore\n",
    "    df['number_of_underscore'] = tweet_column.apply(lambda x: (len(re.findall(r'_', x))))\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF = adding_extra_feature(tweetsDF, tweetsDF[\"Tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>number_of_exclamation</th>\n",
       "      <th>number_of_questionmark</th>\n",
       "      <th>number_of_hashtag</th>\n",
       "      <th>number_of_mention</th>\n",
       "      <th>number_of_quotes</th>\n",
       "      <th>number_of_underscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1916.000000</td>\n",
       "      <td>1916.000000</td>\n",
       "      <td>1916.000000</td>\n",
       "      <td>1916.000000</td>\n",
       "      <td>1916.000000</td>\n",
       "      <td>1916.000000</td>\n",
       "      <td>1916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.074113</td>\n",
       "      <td>0.053758</td>\n",
       "      <td>0.087683</td>\n",
       "      <td>0.163361</td>\n",
       "      <td>0.508873</td>\n",
       "      <td>0.157620</td>\n",
       "      <td>0.106994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.708573</td>\n",
       "      <td>0.307829</td>\n",
       "      <td>0.356418</td>\n",
       "      <td>0.717505</td>\n",
       "      <td>1.155495</td>\n",
       "      <td>0.478465</td>\n",
       "      <td>0.440191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment  number_of_exclamation  number_of_questionmark  \\\n",
       "count  1916.000000            1916.000000             1916.000000   \n",
       "mean      0.074113               0.053758                0.087683   \n",
       "std       0.708573               0.307829                0.356418   \n",
       "min      -1.000000               0.000000                0.000000   \n",
       "25%       0.000000               0.000000                0.000000   \n",
       "50%       0.000000               0.000000                0.000000   \n",
       "75%       1.000000               0.000000                0.000000   \n",
       "max       1.000000               7.000000                6.000000   \n",
       "\n",
       "       number_of_hashtag  number_of_mention  number_of_quotes  \\\n",
       "count        1916.000000        1916.000000       1916.000000   \n",
       "mean            0.163361           0.508873          0.157620   \n",
       "std             0.717505           1.155495          0.478465   \n",
       "min             0.000000           0.000000          0.000000   \n",
       "25%             0.000000           0.000000          0.000000   \n",
       "50%             0.000000           0.000000          0.000000   \n",
       "75%             0.000000           1.000000          0.000000   \n",
       "max            11.000000           9.000000          4.000000   \n",
       "\n",
       "       number_of_underscore  \n",
       "count           1916.000000  \n",
       "mean               0.106994  \n",
       "std                0.440191  \n",
       "min                0.000000  \n",
       "25%                0.000000  \n",
       "50%                0.000000  \n",
       "75%                0.000000  \n",
       "max                5.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, users emoticons in a tweet also matters, so we will find the emoticons in a users tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emoticon Detector\n",
    "\n",
    "class EmoticonDetector:\n",
    "    emoticons = {}\n",
    "\n",
    "    def __init__(self, emoticon_file=\"emoticons.txt\"):\n",
    "        from pathlib import Path\n",
    "        content = Path(emoticon_file).read_text()\n",
    "        positive = True\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if \"positive\" in line.lower():\n",
    "                positive = True\n",
    "                continue\n",
    "            elif \"negative\" in line.lower():\n",
    "                positive = False\n",
    "                continue\n",
    "\n",
    "            self.emoticons[line] = positive\n",
    "\n",
    "    def is_positive(self, emoticon):\n",
    "        if emoticon in self.emoticons:\n",
    "            return self.emoticons[emoticon]\n",
    "        return False\n",
    "\n",
    "    def is_emoticon(self, to_check):\n",
    "        return to_check in self.emoticons\n",
    "ed = EmoticonDetector()\n",
    "\n",
    "processed_data = tweetsDF.copy()\n",
    "\n",
    "def add_column(column_name, column_content):\n",
    "    processed_data.loc[:, column_name] = pd.Series(column_content, index=processed_data.index)\n",
    "\n",
    "def count_by_lambda(expression, word_array):\n",
    "    return len(list(filter(expression, word_array)))\n",
    "\n",
    "add_column(\"splitted_text\", map(lambda txt: txt.split(\" \"), processed_data[\"Tweets\"]))\n",
    "\n",
    "positive_emo = list(\n",
    "    map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and ed.is_positive(word), txt),\n",
    "        processed_data[\"splitted_text\"]))\n",
    "add_column(\"number_of_positive_emo\", positive_emo)\n",
    "\n",
    "negative_emo = list(map(\n",
    "    lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and not ed.is_positive(word), txt),\n",
    "    processed_data[\"splitted_text\"]))\n",
    "\n",
    "add_column(\"number_of_negative_emo\", negative_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_df = processed_data[['number_of_positive_emo', 'number_of_negative_emo']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the emoticons dataframe\n",
    "emoticons_df.to_csv('emoticons_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
